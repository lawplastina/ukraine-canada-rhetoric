match_proportion_ua <- match_count_ua / total_docs_ua
# Output
cat("Matching documents:", match_count_ua, "\n")
cat("Total documents:", total_docs_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
# Create corpus
putin_corpus_int <- corpus(ru_transcripts_putin, text_field = "transcript_filtered")
# Tokenize and remove Russian stopwords
putin_tokens <- tokens(putin_corpus_int, remove_punct = TRUE) |>
tokens_select(pattern = stopwords("ru"), selection = "remove")
# Define target forms of "Украина"
target_words_ua <- c("Украина", "Украины", "Украине", "Украину", "Украиной", "Украине")
# Convert tokens to list of character vectors
putin_token_list <- as.list(putin_tokens)
# Check for presence of any target word in each document
matches_ua <- sapply(putin_token_list, function(doc) any(target_words_ua %in% doc))
# Count and calculate proportion
match_count_ua <- sum(matches_ua)
total_docs_ua <- ndoc(putin_corpus_int)
match_proportion_ua <- match_count_ua / total_docs_ua
# Output results
cat("Matching documents:", match_count_ua, "\n")
cat("Total documents:", total_docs_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
# Define target forms of "Украина"
target_words_ua <- c("украина", "украины", "украине", "украину", "украиной", "украине")
# Convert tokens to list of character vectors
putin_token_list <- as.list(putin_tokens)
# Check for presence of any target word in each document
matches_ua <- sapply(putin_token_list, function(doc) any(target_words_ua %in% doc))
# Count and calculate proportion
match_count_ua <- sum(matches_ua)
total_docs_ua <- ndoc(putin_corpus_int)
match_proportion_ua <- match_count_ua / total_docs_ua
# Output results
cat("Matching documents:", match_count_ua, "\n")
cat("Total documents:", total_docs_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
# Output results
cat("Matching documents:", match_count_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
# Define target forms of "Украина"
target_words_ua <- c("украина", "украины", "украине", "украину", "украиной", "украине",
# Masculine Singular
"украинский", "украинского", "украинскому", "украинского", "украинским", "украинском",
# Feminine Singular
"украинская", "украинской", "украинскую", "украинской", "украинской",
# Neuter Singular
"украинское", "украинского", "украинскому", "украинское", "украинским", "украинском",
# Plural (Masculine Animate Accusative = Genitive)
"украинские", "украинских", "украинским", "украинские", "украинскими", "украинских")
# Convert tokens to list of character vectors
putin_token_list <- as.list(putin_tokens)
# Check for presence of any target word in each document
matches_ua <- sapply(putin_token_list, function(doc) any(target_words_ua %in% doc))
# Count and calculate proportion
match_count_ua <- sum(matches_ua)
total_docs_ua <- ndoc(putin_corpus_int)
match_proportion_ua <- match_count_ua / total_docs_ua
# Output results
cat("Matching documents:", match_count_ua, "\n")
cat("Total documents:", total_docs_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
??get_centroid
source(word_embeddings.Rmd)
source("word_embeddings.Rmd")
source("C:/Users/12015/Desktop/popova_beland/scripts/word_embeddings.Rmd")
source("C:/Users/12015/Desktop/popova_beland/scripts/word_embeddings.Rmd")
source("C:/Users/12015/Desktop/popova_beland/scripts/messing_with_text.R")
knitr::opts_chunk$set(echo = TRUE)
g_model <- GlobalVectors$new(rank = 100, x_max = 20)
# Create our TCM
us_words <- space_tokenizer(us_transcripts$trump_only)
# Create our TCM
us_words <- space_tokenizer(us_transcripts$trump_only)
us_iterate <- itoken(us_words)
us_voc <- create_vocabulary(us_iterate)
us_vocab_vec <- vocab_vectorizer(us_voc)
us_st_tcm <- create_tcm(iterate, us_vocab_vec, skip_grams_window = 5L)
us_st_tcm <- create_tcm(us_iterate, us_vocab_vec, skip_grams_window = 5L)
us_st_tcm <- create_tcm(us_iterate, us_vocab_vec, skip_grams_window = 5L)
us_st_vectors <- g_model$fit_transform(us_st_tcm, n_iter = 50)
us_st_vectors <- us_st_vectors + t(g_model$components)
us_st_2d <- us_st_vectors |>
as.data.frame() |>
mutate(name = rownames(us_st_vectors)) |>
filter(!name %in% get_stoplist("snowball2014")) |>
select(!name)
us_st_2d <- us_st_vectors |>
as.data.frame() |>
mutate(name = rownames(us_st_vectors)) |>
filter(!name %in% get_stoplist("snowball2014")) |>
select(!name)
us_wv_coords <- svd(us_st_2d)$u
us_wv_coords <- us_wv_coords |>
as.data.frame() |>
select(V1, V2) |>
mutate(name = rownames(stus_st_2d_2d))
us_st_2d <- us_st_vectors |>
as.data.frame() |>
mutate(name = rownames(us_st_vectors)) |>
filter(!name %in% get_stoplist("snowball2014")) |>
select(!name)
us_wv_coords <- svd(us_st_2d)$u
us_wv_coords <- us_wv_coords |>
as.data.frame() |>
select(V1, V2) |>
mutate(name = rownames(us_st_2d))
us_wv_coords |>
mutate(col = case_when(
abs(V1) >= 0.03 ~ TRUE,
abs(V2) >= 0.03 ~ TRUE,
TRUE ~ FALSE),
lab = case_when(
abs(V1) >= 0.03 ~ name,
abs(V2) >= 0.03 ~ name,
TRUE ~ NA_character_)) |>
ggplot(aes(V1, V2)) +
geom_point(aes(color = col), alpha = 0.1, show.legend = FALSE) +
geom_text_repel(aes(label = lab),
max.overlaps = Inf, show.legend = FALSE) +
labs(x = "Dimension 1", y = "Dimension 2")
us_wv_coords |>
mutate(col = case_when(
abs(V1) >= 0.05 ~ TRUE,
abs(V2) >= 0.05 ~ TRUE,
TRUE ~ FALSE),
lab = case_when(
abs(V1) >= 0.05 ~ name,
abs(V2) >= 0.05 ~ name,
TRUE ~ NA_character_)) |>
ggplot(aes(V1, V2)) +
geom_point(aes(color = col), alpha = 0.1, show.legend = FALSE) +
geom_text_repel(aes(label = lab),
max.overlaps = Inf, show.legend = FALSE) +
labs(x = "Dimension 1", y = "Dimension 2")
us_wv_coords |>
mutate(col = case_when(
abs(V1) >= 0.02 ~ TRUE,
abs(V2) >= 0.02 ~ TRUE,
TRUE ~ FALSE),
lab = case_when(
abs(V1) >= 0.02 ~ name,
abs(V2) >= 0.02 ~ name,
TRUE ~ NA_character_)) |>
ggplot(aes(V1, V2)) +
geom_point(aes(color = col), alpha = 0.1, show.legend = FALSE) +
geom_text_repel(aes(label = lab),
max.overlaps = Inf, show.legend = FALSE) +
labs(x = "Dimension 1", y = "Dimension 2")
us_wv_coords |>
mutate(col = case_when(
abs(V1) >= 0.025 ~ TRUE,
abs(V2) >= 0.025 ~ TRUE,
TRUE ~ FALSE),
lab = case_when(
abs(V1) >= 0.025 ~ name,
abs(V2) >= 0.025 ~ name,
TRUE ~ NA_character_)) |>
ggplot(aes(V1, V2)) +
geom_point(aes(color = col), alpha = 0.1, show.legend = FALSE) +
geom_text_repel(aes(label = lab),
max.overlaps = Inf, show.legend = FALSE) +
labs(x = "Dimension 1", y = "Dimension 2")
library(uwot)
install.packages("uwot")
??umap
install.packages("uwot")
library(uwot)
remotes::install_github("jlmelville/uwot")
??AalenJohansen
install.packages("AalenJohansen")
remotes::install_github("jlmelville/uwot")
tempdir()
knitr::opts_chunk$set(echo = TRUE)
us_umap <- umap(us_st_2d)
library(uwot)
us_umap <- umap(us_st_2d)
g_model <- GlobalVectors$new(rank = 100, x_max = 20)
knitr::opts_chunk$set(echo = TRUE)
# SAVE DATAFRAME AS R OBJECT AND LOAD IT IN HERE
library("quanteda")
library("spacyr")
library("text2map")
library("text2vec")
library("ggrepel")
library("textclean")
library("textclean")
library("irlba")
# create corpus
trump_corpus_int <- corpus(us_transcripts, text_field = "trump_only")
knitr::opts_chunk$set(echo = TRUE)
library(ndjson)
library(dplyr)
library(stringr)
library(lubridate)
library(tidyr)
library(jsonlite)
library(xml2)
library(rvest)
setwd("C:/Users/12015/Desktop/popova_beland")
# Read NDJSON file into R
us_transcripts_unfiltered <- ndjson::stream_in("C:/Users/12015/Desktop/popova_beland/output_transcripts/all_transcripts_with_officials.ndjson")
us_transcripts_mid <- as.data.frame(us_transcripts_unfiltered) %>%
mutate(
type = str_extract(title, "^[^:]+"),
title = str_trim(str_remove(title, "^[^:]+:\\s*")),
transcript = if_else(str_starts(title, "No Transcript -"), "No", "Yes"),
title = str_remove(title, "^No Transcript -\\s*"),  # removes the prefix if present
date_parsed = ymd(date), # converting to Date object
trump_only = na_if(trump_only, ""), # entering NAs for all blank entries
homan_only = na_if(homan_only, ""),
leavitt_only = na_if(leavitt_only, ""),
rubio_only = na_if(rubio_only, ""),
miller_only = na_if(miller_only, "")
) %>%
drop_na(date_parsed) %>% # removing the one South African observation with no date
filter(date_parsed <= as.Date("2025-06-01")) %>% # filtering everything after May 31st
select(-date) # getting rid of the character variable bc it's not useful
# Now pulling in Truth Social posts
# Useful function:
html_to_text <- function(x) {
doc <- read_html(x)
html_text(doc, trim = TRUE)
}
trump_transcripts_unfiltered <- read.csv("C:/Users/12015/Desktop/popova_beland/trump_truth_fuller.csv")
trump_transcripts <- trump_transcripts_unfiltered %>%
mutate(date_parsed_sec = ymd_hms(created_at),
date_parsed = date(date_parsed_sec),
type = "Truth Social") %>%
filter(date_parsed <= as.Date("2025-06-01")) %>%
mutate(trump_only = sapply(content, html_to_text),
trump_only =
str_remove_all(trump_only, "https://[^\\s\"'>]+"), #OPTIONAL: removing posts with only links
# Note: also filter out posts like "RT:", "USGB", etc?
trump_only = na_if(trump_only, "")) %>%
select(-created_at, -date_parsed_sec, -id, -quote_id,
-in_reply_to_id, -in_reply_to_account_id, -content) %>%
filter(!is.na(trump_only))
us_transcripts <- full_join(us_transcripts_mid, trump_transcripts) %>%
filter(date_parsed >= as.Date("2024-11-05"))
ru_transcripts_unfiltered <- ndjson::stream_in(normalizePath("C:/Users/12015/Desktop/popova_beland/kremlin/kremlin_transcripts.json"))
ru_transcripts_mid <- ru_transcripts_unfiltered %>%
mutate(date_parsed = ymd_hms(date),
teaser = na_if(teaser, ""),
transcript_filtered = na_if(transcript_filtered, ""),
type = "kremlin",
person = "putin") %>% #IS THIS RIGHT?? CHECK
select(-starts_with(c("wordlist", "tags", "persons")), -place, -date)
zakharova_unfiltered <- read.csv("C:/Users/12015/Desktop/popova_beland/MariaVladimirovnaZakharova/MariaVladimirovnaZakharova.csv")
zakharova <- zakharova_unfiltered %>%
mutate(date_parsed = ymd_hms(date),
message = na_if(message, ""),
media_type = na_if(media_type, ""),
type = "telegram",
person = "zakharova",
transcript_filtered_int = message,
transcript_filtered =
str_remove_all(transcript_filtered_int, "https://[^\\s\"'>]+"),
transcript_filtered = na_if(transcript_filtered, "")
) %>%
select(-first_name, -last_name, -username,
-media_path, -reply_to, -date, -message_id,
-media_type, -id, -sender_id, -message, -transcript_filtered_int) %>%
filter(!is.na(transcript_filtered))
# Now attaching Zakharova df to ru_transcripts
ru_transcripts <- full_join(ru_transcripts_mid, zakharova)
knitr::opts_chunk$set(echo = TRUE)
# SAVE DATAFRAME AS R OBJECT AND LOAD IT IN HERE
library("quanteda")
library("spacyr")
library("text2map")
library("text2vec")
library("ggrepel")
library("textclean")
library("textclean")
library("irlba")
# create corpus
trump_corpus_int <- corpus(us_transcripts, text_field = "trump_only")
trump_corpus <- tokens(trump_corpus_int, remove_punct = TRUE) |>
tokens_tolower() |>
tokens_select(
pattern = quanteda::stopwords("en"),
selection = "remove"
)
# Define target words
target_words <- c("canada", "canadian", "canadians")
matches <- sapply(trump_corpus, function(doc) any(target_words %in% doc))
# Number of matching documents
match_count <- sum(matches)
# Total number of documents
total_docs <- ndoc(trump_corpus)
# Proportion
match_proportion <- match_count / total_docs
# Output
cat("Matching documents:", match_count, "\n")
cat("Total documents:", total_docs, "\n")
cat("Proportion:", round(match_proportion * 100, 2), "%\n")
us_transcripts <- us_transcripts |>
mutate(
trump_only = stringi::stri_trans_general(trump_only, "Any-Latin; Latin-ASCII"),
trump_only = replace_contraction(trump_only),
trump_only = tolower(trump_only),
trump_only = gsub("[[:punct:]]+", " ", trump_only),
trump_only = gsub("\\s+", " ", trump_only)
)
tcm_trump <- us_transcripts$trump_only |>
tokens() |>
fcm(context = "window", window = 10) |>
as("dgCMatrix")
# Only keeping words with at least 30 co-occurrences (can maybe edit later?)
tcm_trump <- tcm_trump[colSums(tcm_trump) > 30, colSums(tcm_trump) > 30]
weight_ppmi <- function(tcm) { ## fix zero self-occurrences
diag(tcm) <- diag(tcm) + 1 ## weight by PMI
tcm <- tcm %*% diag(1 / diag(tcm))
tcm <- log(tcm) ## positive PMI
tcm[tcm < 0] <- 0
return(Matrix(tcm, sparse = TRUE)) }
tcm_trump_ppmi <- weight_ppmi(tcm_trump)
svd_tng_trump <- irlba(tcm_trump_ppmi, nv = 100)
wv_tng_trump <- svd_tng_trump$v[, 1:100]
rownames(wv_tng_trump) <- rownames(tcm_trump_ppmi)
canada_vec <- c("canada")
can_cent <- get_centroid(canada_vec, wv_tng_trump, missing = "stop")
can_sims <- sim2(wv_tng_trump, can_cent, method = "cosine")
names(head(sort(can_sims[,1], decreasing = TRUE), n = 50L))
# focal <- c("american", "canadian") #This is sensitive to tense and number, remember!!
#
# vecs <- wv_tng_trump[focal, ]
# sims <- sim2(vecs, wv_tng_trump, method = "cosine")
# df_sims <- data.frame( word1 = sims[1, ], word2 = sims[2, ], term = colnames(sims) )
#
# df_plot <- df_sims |>
#   mutate(bias = word1 - word2,
#          party = ifelse(bias > 0, "american", "canadian"),
#          bias = abs(bias)
#          )
#
# df_plot |>
#   group_by(party) |>
#   slice_max(bias, n = 30) |>
#   mutate(term = forcats::fct_reorder(term, bias)) |>
#   ggplot(aes(term, bias, fill = party, label = term)) +
#   geom_col() +
#   coord_flip() +
#   facet_wrap(~party, scale = "free")
# These results aren't super interesting...
# Though just intuitively it doesn't seem like there are words
# that Trump *exclusively* uses when discussing Canada.
# Like he talks about "deals" and "states" and "borders"
# all the time, just in different contexts.
# More cleaning
ru_transcripts_putin <- ru_transcripts %>%
filter(person == "putin")|>
mutate(
# transcript_filtered = stringi::stri_trans_general(transcript_filtered, "Any-Latin; Latin-ASCII"),
# transcript_filtered = replace_contraction(transcript_filtered),
transcript_filtered = tolower(transcript_filtered),
transcript_filtered = gsub("[[:punct:]]+", " ", transcript_filtered),
transcript_filtered = gsub("\\s+", " ", transcript_filtered)
)
tcm_putin <- ru_transcripts_putin$transcript_filtered |>
tokens() |>
fcm(context = "window", window = 10) |>
as("dgCMatrix")
# Only keeping words with at least 30 co-occurrences
tcm_putin <- tcm_putin[colSums(tcm_putin) > 30, colSums(tcm_putin) > 30]
tcm_putin_ppmi <- weight_ppmi(tcm_putin)
svd_tng_putin <- irlba(tcm_putin_ppmi, nv = 100)
# word value matrix
wv_tng_putin <- svd_tng_putin$v[, 1:100]
rownames(wv_tng_putin) <- rownames(tcm_putin_ppmi)
# Load quanteda if not already loaded
library(quanteda)
# Create corpus
putin_corpus_int <- corpus(ru_transcripts_putin, text_field = "transcript_filtered")
# Tokenize and remove Russian stopwords
putin_tokens <- tokens(putin_corpus_int, remove_punct = TRUE) |>
tokens_select(pattern = stopwords("ru"), selection = "remove")
# Define target forms of "Украина"
target_words_ua <- c("украина", "украины", "украине", "украину", "украиной", "украине",
# Masculine Singular
"украинский", "украинского", "украинскому", "украинского", "украинским", "украинском",
# Feminine Singular
"украинская", "украинской", "украинскую", "украинской", "украинской",
# Neuter Singular
"украинское", "украинского", "украинскому", "украинское", "украинским", "украинском",
# Plural (Masculine Animate Accusative = Genitive)
"украинские", "украинских", "украинским", "украинские", "украинскими", "украинских")
# Convert tokens to list of character vectors
putin_token_list <- as.list(putin_tokens)
# Check for presence of any target word in each document
matches_ua <- sapply(putin_token_list, function(doc) any(target_words_ua %in% doc))
# Count and calculate proportion
match_count_ua <- sum(matches_ua)
total_docs_ua <- ndoc(putin_corpus_int)
match_proportion_ua <- match_count_ua / total_docs_ua
# Output results
cat("Matching documents:", match_count_ua, "\n")
cat("Total documents:", total_docs_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
ukraine_vec <- c("украина",
"украины",
"украине",
"украину")
ua_cent <- get_centroid(ukraine_vec, wv_tng_putin, missing = "stop")
ua_sims <- sim2(wv_tng_putin, ua_cent, method = "cosine")
names(head(sort(ua_sims[,1], decreasing = TRUE), n = 50L))
# focal_rus <- c("русский", "украинский") #This is sensitive to tense and number, remember!!
#
# vecs_rus <- wv_tng_putin[focal_rus, ]
#
# sims_rus <- sim2(vecs_rus, wv_tng_putin, method = "cosine")
#
# df_sims_rus <- data.frame( word1 = sims_rus[1, ], word2 = sims_rus[2, ], term = colnames(sims_rus) )
#
# df_plot_rus <- df_sims_rus |>
#   mutate(bias = word1 - word2,
#          nation = ifelse(bias > 0, "русский", "украинский"),
#          bias = abs(bias)
#          )
#
# df_plot_rus |>
#   group_by(nation) |>
#   slice_max(bias, n = 30) |>
#   mutate(term = forcats::fct_reorder(term, bias)) |>
#   ggplot(aes(term, bias, fill = nation, label = term)) +
#   geom_col() +
#   coord_flip() +
#   facet_wrap(~nation, scale = "free")
# THIS WHOLE SCRIPT REQUIRES RUNNING word_embeddings.Rmd FIRST PLEASE
g_model <- GlobalVectors$new(rank = 100, x_max = 20)
# Create our TCM
us_words <- space_tokenizer(us_transcripts$trump_only)
us_iterate <- itoken(us_words)
us_voc <- create_vocabulary(us_iterate)
us_vocab_vec <- vocab_vectorizer(us_voc)
us_st_tcm <- create_tcm(us_iterate, us_vocab_vec, skip_grams_window = 5L)
us_st_vectors <- g_model$fit_transform(us_st_tcm, n_iter = 50)
us_st_vectors <- us_st_vectors + t(g_model$components)
us_st_2d <- us_st_vectors |>
as.data.frame() |>
mutate(name = rownames(us_st_vectors)) |>
filter(!name %in% get_stoplist("snowball2014")) |>
select(!name)
us_wv_coords <- svd(us_st_2d)$u
us_wv_coords <- us_wv_coords |>
as.data.frame() |>
select(V1, V2) |>
mutate(name = rownames(us_st_2d))
us_wv_coords |>
mutate(col = case_when(
abs(V1) >= 0.025 ~ TRUE,
abs(V2) >= 0.025 ~ TRUE,
TRUE ~ FALSE),
lab = case_when(
abs(V1) >= 0.025 ~ name,
abs(V2) >= 0.025 ~ name,
TRUE ~ NA_character_)) |>
ggplot(aes(V1, V2)) +
geom_point(aes(color = col), alpha = 0.1, show.legend = FALSE) +
geom_text_repel(aes(label = lab),
max.overlaps = Inf, show.legend = FALSE) +
labs(x = "Dimension 1", y = "Dimension 2")
library(uwot)
us_umap <- umap(us_st_2d)
us_plot_df <- as.data.frame(us_umap)
us_plot_df$name <- rownames(us_st_2d)
us_plot_df
us_umap <- umap(us_st_2d, n_neighbors = 15, min_dist = 0.1, metric = "cosine")
us_plot_df$name <- rownames(us_st_2d)
ggplot(umap_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 100) +
labs(x = "UMAP 1", y = "UMAP 2")
ggplot(us_plot_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 100) +
labs(x = "UMAP 1", y = "UMAP 2")
ggplot(us_plot_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 50) +
labs(x = "UMAP 1", y = "UMAP 2")
ggplot(us_plot_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 200) +
labs(x = "UMAP 1", y = "UMAP 2")
ggplot(us_plot_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 150) +
labs(x = "UMAP 1", y = "UMAP 2")
ggplot(us_plot_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 175) +
labs(x = "UMAP 1", y = "UMAP 2")
ggplot(us_plot_df, aes(V1, V2)) +
geom_point(alpha = 0.2) +
geom_text_repel(aes(label = name), max.overlaps = 185) +
labs(x = "UMAP 1", y = "UMAP 2")
download_pretrained("vecs_fasttext300_commoncrawl")
??download_pretrained
??fastText
text2map::download_pretrained("vecs_fasttext300_commoncrawl")
data("vecs_fasttext300_commoncrawl", package ="text2map.pretrained")
data("vecs_fasttext300_commoncrawl", package ="text2map.pretrained")
```{r}
library(text2vec)
# Unzip it first, then load the .vec file
vec_file <- "crawl-300d-2M.vec"  # path to the unzipped file
wv <- read.wordvectors(vec_file, type = "text", normalize = TRUE)
wv <- text2vec::read.wordvectors(vec_file, type = "text", normalize = TRUE)
??read.wordvectors
