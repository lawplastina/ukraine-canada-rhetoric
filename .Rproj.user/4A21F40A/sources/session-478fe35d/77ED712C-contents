---
title: "Fun with Text"
author: "Lawrence Plastina"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# SAVE DATAFRAME AS R OBJECT AND LOAD IT IN HERE

library("quanteda")
library("spacyr")
library("text2map") 
library("text2vec")
library("ggrepel")
library("textclean")
library("textclean")
library("irlba")

# create corpus
trump_corpus_int <- corpus(us_transcripts, text_field = "trump_only") 

trump_corpus <- tokens(trump_corpus_int, remove_punct = TRUE) |>
  tokens_tolower() |>
  tokens_select(
        pattern = quanteda::stopwords("en", source = "snowball"), 
        selection = "remove"
    )

full_corpus_int <- corpus(us_transcripts, text_field = "full_transcript")

full_corpus <- tokens(full_corpus_int, remove_punct = TRUE) |>
  tokens_tolower() |>
  tokens_select(
        pattern = quanteda::stopwords("en", source = "snowball"),
        selection = "remove"
    )
```

## How many documents even mention Canada

```{r}
# Define target words
target_words <- c("canada", "canadian", "canadians")

matches <- sapply(trump_corpus, function(doc) any(target_words %in% doc))

matches_full <- sapply(full_corpus, function(doc) any(target_words %in% doc))

# Number of matching documents
match_count <- sum(matches)
match_count_full <- sum(matches_full)

# Total number of documents
total_docs <- ndoc(trump_corpus)

# Proportion
match_proportion <- match_count / total_docs
match_proportion_full <- match_count_full / total_docs

# Output
cat("Matching documents:", match_count_full, "\n")
cat("Total documents:", total_docs, "\n")
cat("Proportion:", round(match_proportion_full * 100, 2), "%\n")

us_transcripts_canada <- us_transcripts %>%
  filter(matches_full)

write.csv(us_transcripts_canada["full_transcript"], "C:/Users/12015/Desktop/popova_beland/data_clean/canada_mentions.csv", row.names = FALSE)

# Combine date and text per observation
combined_lines <- paste0(
  "Date: ", us_transcripts_canada$date_parsed, "\n\n",
  us_transcripts_canada$full_transcript, "\n\n",
  strrep("-", 40), "\n"
)

writeLines(combined_lines, "C:/Users/12015/Desktop/popova_beland/data_clean/canada_mentions.txt")

```

## Cleaning and WVM calculation

```{r}
us_transcripts <- us_transcripts |> 
  mutate(
    trump_only = stringi::stri_trans_general(trump_only, "Any-Latin; Latin-ASCII"),
    trump_only = replace_contraction(trump_only),
    trump_only = tolower(trump_only),
    trump_only = gsub("[[:punct:]]+", " ", trump_only),
    trump_only = gsub("\\s+", " ", trump_only)
  )

tcm_trump <- us_transcripts$trump_only |> 
  tokens() |> 
  fcm(context = "window", window = 10) |> 
  as("dgCMatrix")

# Only keeping words with at least 30 co-occurrences (can maybe edit later?)
tcm_trump <- tcm_trump[colSums(tcm_trump) > 30, colSums(tcm_trump) > 30]

weight_ppmi <- function(tcm) { ## fix zero self-occurrences 
  diag(tcm) <- diag(tcm) + 1 ## weight by PMI 
  tcm <- tcm %*% diag(1 / diag(tcm)) 
  tcm <- log(tcm) ## positive PMI 
  tcm[tcm < 0] <- 0 
  return(Matrix(tcm, sparse = TRUE)) }

tcm_trump_ppmi <- weight_ppmi(tcm_trump)

svd_tng_trump <- irlba(tcm_trump_ppmi, nv = 100)

wv_tng_trump <- svd_tng_trump$v[, 1:100]
rownames(wv_tng_trump) <- rownames(tcm_trump_ppmi)
```

## Alternative WVM 

## Semantic centroids

Here I shall explain what the hell I am doing with the centroids. 

From Stoltz and Taylor 2024: "Related to the semantic direction is the semantic centroid. A centroid is an averaged vector. Semantic centroids are useful on their own if we want to know what the discursive context of some domain of interest is, rather than the context of that domain relative to another (which is what we get with directions)."

"A good way to think about the difference between directions and centroids is that directions are better for picking up on relational biases (i.e., terms that favor one pole at the exclusion of the other), while centroids are better for identifying general contexts or the intersection of contexts. Ultimately, though, this depends on what meanings we are attempting to measure. Some meanings are tightly organized into binary spectra; others are not. A more theoretical way to consider the contrast between centroids and directions is using the linguist Gremais “semiotic square” (Greimas 1983). Gremais organized meanings in terms of four poles. In our example, these would be “immigration” and “citizenship,” as well as “not-immigration” and “not-citizenship.” While semantic directions get at immigration/citizenship juxtaposition, centroids get closer to immigration/not-immigration or citizenship/not-citizenship oppositions."


```{r}
canada_vec <- c("canada")

can_cent <- get_centroid(canada_vec, wv_tng_trump, missing = "stop")

can_sims <- sim2(wv_tng_trump, can_cent, method = "cosine")

names(head(sort(can_sims[,1], decreasing = TRUE), n = 50L))
```

## Semantic directions (necessary?)

```{r}
# focal <- c("american", "canadian") #This is sensitive to tense and number, remember!!
# 
# vecs <- wv_tng_trump[focal, ] 
# sims <- sim2(vecs, wv_tng_trump, method = "cosine") 
# df_sims <- data.frame( word1 = sims[1, ], word2 = sims[2, ], term = colnames(sims) )
# 
# df_plot <- df_sims |> 
#   mutate(bias = word1 - word2, 
#          party = ifelse(bias > 0, "american", "canadian"), 
#          bias = abs(bias) 
#          ) 
# 
# df_plot |> 
#   group_by(party) |> 
#   slice_max(bias, n = 30) |> 
#   mutate(term = forcats::fct_reorder(term, bias)) |> 
#   ggplot(aes(term, bias, fill = party, label = term)) +
#   geom_col() + 
#   coord_flip() + 
#   facet_wrap(~party, scale = "free")

# These results aren't super interesting...
# Though just intuitively it doesn't seem like there are words
# that Trump *exclusively* uses when discussing Canada.
# Like he talks about "deals" and "states" and "borders"
# all the time, just in different contexts.
```


# Russia

## Cleaning and WVM calculation

```{r}
# More cleaning

ru_transcripts_putin <- ru_transcripts %>%
  filter(person == "putin")|> 
  mutate(
    # transcript_filtered = stringi::stri_trans_general(transcript_filtered, "Any-Latin; Latin-ASCII"),
    # transcript_filtered = replace_contraction(transcript_filtered),
    transcript_filtered = tolower(transcript_filtered),
    transcript_filtered = gsub("[[:punct:]]+", " ", transcript_filtered),
    transcript_filtered = gsub("\\s+", " ", transcript_filtered)
  )

tcm_putin <- ru_transcripts_putin$transcript_filtered |> 
  tokens() |> 
  fcm(context = "window", window = 10) |> 
  as("dgCMatrix")

# Only keeping words with at least 30 co-occurrences 

tcm_putin <- tcm_putin[colSums(tcm_putin) > 30, colSums(tcm_putin) > 30]

tcm_putin_ppmi <- weight_ppmi(tcm_putin)

svd_tng_putin <- irlba(tcm_putin_ppmi, nv = 100)

# word value matrix

wv_tng_putin <- svd_tng_putin$v[, 1:100]
rownames(wv_tng_putin) <- rownames(tcm_putin_ppmi)
```

## Ukrainian texts as proportion

```{r}
# Load quanteda if not already loaded
library(quanteda)

# Create corpus
putin_corpus_int <- corpus(ru_transcripts_putin, text_field = "transcript_filtered") 

# Tokenize and remove Russian stopwords
putin_tokens <- tokens(putin_corpus_int, remove_punct = TRUE) |>
  tokens_select(pattern = stopwords("ru", source = "snowball"), 
               selection = "remove")

# Define target forms of "Украина"
target_words_ua <- c("украина", "украины", "украине", "украину", "украиной", "украине",
                       # Masculine Singular
  "украинский", "украинского", "украинскому", "украинского", "украинским", "украинском",

  # Feminine Singular
  "украинская", "украинской", "украинскую", "украинской", "украинской",

  # Neuter Singular
  "украинское", "украинского", "украинскому", "украинское", "украинским", "украинском",

  # Plural (Masculine Animate Accusative = Genitive)
  "украинские", "украинских", "украинским", "украинские", "украинскими", "украинских")

# Convert tokens to list of character vectors
putin_token_list <- as.list(putin_tokens)

# Check for presence of any target word in each document
matches_ua <- sapply(putin_token_list, function(doc) any(target_words_ua %in% doc))

# Count and calculate proportion
match_count_ua <- sum(matches_ua)
total_docs_ua <- ndoc(putin_corpus_int)
match_proportion_ua <- match_count_ua / total_docs_ua

# Output results
cat("Matching documents:", match_count_ua, "\n")
cat("Total documents:", total_docs_ua, "\n")
cat("Proportion:", round(match_proportion_ua * 100, 2), "%\n")
```

## Semantic centroids

```{r}
ukraine_vec <- c("украина", 
                 "украины",
                 "украине",
                 "украину")

ua_cent <- get_centroid(ukraine_vec, wv_tng_putin, missing = "stop")

ua_sims <- sim2(wv_tng_putin, ua_cent, method = "cosine")

names(head(sort(ua_sims[,1], decreasing = TRUE), n = 50L))
```

## Semantic directions (necessary?)

```{r}
# focal_rus <- c("русский", "украинский") #This is sensitive to tense and number, remember!!
# 
# vecs_rus <- wv_tng_putin[focal_rus, ] 
# 
# sims_rus <- sim2(vecs_rus, wv_tng_putin, method = "cosine") 
# 
# df_sims_rus <- data.frame( word1 = sims_rus[1, ], word2 = sims_rus[2, ], term = colnames(sims_rus) )
# 
# df_plot_rus <- df_sims_rus |> 
#   mutate(bias = word1 - word2, 
#          nation = ifelse(bias > 0, "русский", "украинский"), 
#          bias = abs(bias) 
#          ) 
# 
# df_plot_rus |> 
#   group_by(nation) |> 
#   slice_max(bias, n = 30) |> 
#   mutate(term = forcats::fct_reorder(term, bias)) |> 
#   ggplot(aes(term, bias, fill = nation, label = term)) +
#   geom_col() + 
#   coord_flip() + 
#   facet_wrap(~nation, scale = "free")
```
