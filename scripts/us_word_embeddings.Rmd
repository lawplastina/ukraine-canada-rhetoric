---
title: "US DTM Calculation"
author: "Lawrence Plastina"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# SAVE DATAFRAME AS R OBJECT AND LOAD IT IN HERE

library("quanteda")
library("spacyr")
library("text2map") 
library("text2vec")
library("ggrepel")
library("textclean")
library("textclean")
library("irlba")

# create corpus
trump_corpus_int <- corpus(us_transcripts, text_field = "trump_only") 

trump_corpus <- tokens(trump_corpus_int, remove_punct = TRUE) |>
  tokens_tolower() |>
  tokens_select(
        pattern = quanteda::stopwords("en"),
        selection = "remove"
    )

full_corpus_int <- corpus(us_transcripts, text_field = "full_transcript")

full_corpus <- tokens(full_corpus_int, remove_punct = TRUE) |>
  tokens_tolower() |>
  tokens_select(
        pattern = quanteda::stopwords("en"),
        selection = "remove"
    )
```

## Cleaning and WVM calculation

```{r}
us_transcripts <- us_transcripts |> 
  mutate(
    trump_only = stringi::stri_trans_general(trump_only, "Any-Latin; Latin-ASCII"),
    trump_only = replace_contraction(trump_only),
    trump_only = tolower(trump_only),
    trump_only = gsub("[[:punct:]]+", " ", trump_only),
    trump_only = gsub("\\s+", " ", trump_only)
  )

tcm_trump <- us_transcripts$trump_only |> 
  tokens() |> 
  fcm(context = "window", window = 10) |> 
  as("dgCMatrix")

# Only keeping words with at least 30 co-occurrences (can maybe edit later?)
tcm_trump <- tcm_trump[colSums(tcm_trump) > 30, colSums(tcm_trump) > 30]

weight_ppmi <- function(tcm) { ## fix zero self-occurrences 
  diag(tcm) <- diag(tcm) + 1 ## weight by PMI 
  tcm <- tcm %*% diag(1 / diag(tcm)) 
  tcm <- log(tcm) ## positive PMI 
  tcm[tcm < 0] <- 0 
  return(Matrix(tcm, sparse = TRUE)) }

tcm_trump_ppmi <- weight_ppmi(tcm_trump)

svd_tng_trump <- irlba(tcm_trump_ppmi, nv = 100)

wv_tng_trump <- svd_tng_trump$v[, 1:100]
rownames(wv_tng_trump) <- rownames(tcm_trump_ppmi)
```

## Alternative WVM 

## Semantic centroids

Here I shall explain what the hell I am doing with the centroids. 

From Stoltz and Taylor 2024: "Related to the semantic direction is the semantic centroid. A centroid is an averaged vector. Semantic centroids are useful on their own if we want to know what the discursive context of some domain of interest is, rather than the context of that domain relative to another (which is what we get with directions)."

"A good way to think about the difference between directions and centroids is that directions are better for picking up on relational biases (i.e., terms that favor one pole at the exclusion of the other), while centroids are better for identifying general contexts or the intersection of contexts. Ultimately, though, this depends on what meanings we are attempting to measure. Some meanings are tightly organized into binary spectra; others are not. A more theoretical way to consider the contrast between centroids and directions is using the linguist Gremais “semiotic square” (Greimas 1983). Gremais organized meanings in terms of four poles. In our example, these would be “immigration” and “citizenship,” as well as “not-immigration” and “not-citizenship.” While semantic directions get at immigration/citizenship juxtaposition, centroids get closer to immigration/not-immigration or citizenship/not-citizenship oppositions."


```{r}
canada_vec <- c("canada")

can_cent <- get_centroid(canada_vec, wv_tng_trump, missing = "stop")

can_sims <- sim2(wv_tng_trump, can_cent, method = "cosine")

names(head(sort(can_sims[,1], decreasing = TRUE), n = 50L))
```

## Semantic directions (necessary?)

```{r}
# focal <- c("american", "canadian") #This is sensitive to tense and number, remember!!
# 
# vecs <- wv_tng_trump[focal, ] 
# sims <- sim2(vecs, wv_tng_trump, method = "cosine") 
# df_sims <- data.frame( word1 = sims[1, ], word2 = sims[2, ], term = colnames(sims) )
# 
# df_plot <- df_sims |> 
#   mutate(bias = word1 - word2, 
#          party = ifelse(bias > 0, "american", "canadian"), 
#          bias = abs(bias) 
#          ) 
# 
# df_plot |> 
#   group_by(party) |> 
#   slice_max(bias, n = 30) |> 
#   mutate(term = forcats::fct_reorder(term, bias)) |> 
#   ggplot(aes(term, bias, fill = party, label = term)) +
#   geom_col() + 
#   coord_flip() + 
#   facet_wrap(~party, scale = "free")

# These results aren't super interesting...
# Though just intuitively it doesn't seem like there are words
# that Trump *exclusively* uses when discussing Canada.
# Like he talks about "deals" and "states" and "borders"
# all the time, just in different contexts.
```

## Trying out Word Mover's Distance

```{r}
# data("contractions", package = "qdapDictionaries") 
# install.packages("qdapDictionaries")

df_ittpr <- corpus_ittpr |> 
mutate(text = replace_non_ascii(text), 
text = replace_url(text, replacement = " "), 
text = replace_html(text, replacement = " "), 
# text = replace_contraction(text, contraction.key = contractions, ignore.case = TRUE), 
text = str_replace_all(text, "[[:punct:]]", " "), 
text = replace_ordinal(text), 
text = str_replace_all(text, "[\\s]+", " "), 
text = tolower(text) 
) 

dtm_ittpr <- df_ittpr |> 
dtm_builder(text, doc_id) |> 
dtm_stopper(stop_list = get_stoplist("snowball2014"), stop_docprop = c(.01, Inf))
```

